# Test Training
algo: "PPO"
policy: MlpPolicy

# Step settings
n_steps: 1024
max_ep_steps: 512
frame_skip: 1

total_timesteps: 10_000_000
save_interval: 1_000_000
eval_freq: 500_000
n_eval_ep: 100

learning_rate: 0.0003


# Reward setup
healthy_reward: 1

y_angle_pen: 0.2
y_angle_scale: 1.0

z_angle_pen: 0.025
z_angle_scale: 1.0

dist_pen: 0.2
dist_scale: 15.0

wheel_speed_pen: 0.1
wheel_speed_scale: 1.0

x_vel_pen: 0.45
x_vel_scale: 15.0

y_angle_vel_pen: 0.025
y_angle_vel_scale: 1.0

# Angle and height settings
max_angle: 10.0
rigid: False
height_level: 1.0

# Force perturbation settings
first_disturbance: 200
duration_disturbance: 5
max_disturbance: 100 # in N

# Curriculum learning (x takes values in the range [0, 1], the output will be clamped to [0, 1]

# difficulty_schedule: "lin"  # grad * (x + x_offset)
# difficulty_schedule: "exp"  # exp(grad * x + x_offset)
# difficulty_schedule: "cub"  # grad * (x + x_offset)^3
difficulty_schedule: "sig"  # 1 / (1 + e^(-grad * (x + x_offset)))
difficulty_grad: 9.0
difficulty_x_offset: -0.35
difficulty_start: 0.0