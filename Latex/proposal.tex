\documentclass[a4paper]{article}
\usepackage[a4paper,left=3cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\hypersetup{colorlinks, urlcolor=blue}

\title{Learning Balance Control of a Wheel-legged Robot}
\author{Noah Böckmann, Felix Weidenmüller, Lino Willenbrink}
\date{\today}

\makeatletter
\DeclareUrlCommand\ULurl@@{%
  \def\UrlFont{\ttfamily\color{blue}}%
  \def\UrlLeft{\uline\bgroup}%
  \def\UrlRight{\egroup}}
\def\ULurl@#1{\hyper@linkurl{\ULurl@@{#1}}{#1}}
\DeclareRobustCommand*\ULurl{\hyper@normalise\ULurl@}
\makeatother

\begin{document}

\maketitle
\section{Problem Description}
Wheel-legged robots combine the energy efficiency of wheels with the adaptability of legs and
therefore exhibit significant potential in applications requiring agility and terrain versatility.
However, their inherently unstable structure introduces unique difficulties, especially under
conditions of nonlinearities, uncertainties, and dynamic posture changes like height adjustments.
In the context of balancing a wheel-legged robot, the primary challenge lies in achieving and
maintaining stability.

We are especially interested in how this problem can be addressed by a machine learning approach.
Our primary aim is to stabilize the robot in the standing equilibrium position by replacing the LQ
regulator with a trained policy, thus circumventing the tedious design of a mathematical model.

As a secondary goal we want to allow variations/uncertainty in the center of gravity and varying the
robot height on the fly.

\section{Relevant Work}

\section{Research Plan}
Our plan to tackle this problem has four steps which will be described in the following:
\begin{enumerate}
  \item Setup of the simulation environment and integration with the machine learning framework.
  \item Defining the geometry of the robot and training a proof of concept controller for a simple
        and static robot state
  \item Training a controller to keep the robot in its upright position while allowing for changing heights
  \item Improving the controller further by introducing perturbations such as shifting the center of
        gravity.
\end{enumerate}

The setup of the simulation environment will probably utilize pybullet or gazebo as a simulation
environment. Here an interface for controlling the bot and interacting with the simulation environment
will be needed. On top of that an integration with Gymnasium, a popular reinforcement learning
framework will be needed. This involves setting up a gym environment and implementing the necessary
feedback mechanisms for calculating the reward function.

With the learning environment set up the next step is to define the geometry and capabilities of the
bot, which can be found in the paper referenced above. With a basic reward function we can then try
to train a policy which is able to keep the robot upright, just like an inverted pendulum.

After this baseline is established we will move forward to training a more general policy aiming for
stability in different height configurations of the robot.

In the end we might be able to extend our goals and try to make the controller robust by training
the policy on a mildly randomized center of gravity and actively changing robot heights and possibly
other features.
\end{document}