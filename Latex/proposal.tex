\documentclass[a4paper]{article}
\usepackage[a4paper,left=3cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\hypersetup{colorlinks, urlcolor=blue}

\title{Learning Balance Control of a Wheel-legged Robot}
\author{Noah Böckmann, Felix Weidenmüller, Lino Willenbrink}
\date{\today}

\makeatletter
\DeclareUrlCommand\ULurl@@{%
  \def\UrlFont{\ttfamily\color{blue}}%
  \def\UrlLeft{\uline\bgroup}%
  \def\UrlRight{\egroup}}
\def\ULurl@#1{\hyper@linkurl{\ULurl@@{#1}}{#1}}
\DeclareRobustCommand*\ULurl{\hyper@normalise\ULurl@}
\makeatother

\begin{document}

\maketitle
\section{Problem Description}
Wheel-legged robots combine the energy efficiency of wheels with the adaptability of legs and therefore exhibit significant potential in applications requiring agility and terrain versatility.
However, their inherently unstable structure introduces unique difficulties, especially under conditions of nonlinearities, uncertainties, and dynamic posture changes like height adjustments.
In the context of balancing a wheel-legged robot, the primary challenge lies in achieving and maintaining stability.

This problem is crucial as it directly impacts the robot's practical usability in real-world scenarios.
Without effective balance control, the robot's ability to perform more complex tasks or navigate uneven terrain is compromised, limiting its application in industry.
% Addressing these challenges can lead to breakthroughs in robotic control systems and unlock greater efficiency and functionality for wheel-legged robots.

We are especially interested in how effective this problem can be addressed by a machine learning approach, and to compare the outcome with the results presented in the original paper with the linear regulator.
Our primary aim is to stabilize the robot in the standing equilibrium position by replacing the used LQR regulator. Additionally, we allow variations/uncertainty in the center of gravity and varying the robot height by changing the leg angle.
If time permits, the implementation of a function for moving the robot would be conceivable.


\section{Relevant Work}
We directly base our project on the work done by S. Wang et al., ``Balance Control of a Novel Wheel-legged Robot: Design and Experiments,'' 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021, pp. 6782-6788, doi: 10.1109/ICRA48506.2021.9561579. In the paper mentioned, the novel wheel-legged robot as well as linear and non-linear controllers for stabilization are introduced and validated through experiments.
... \newline

Mark Towers, undefined., et al, "Gymnasium: A Standard Interface for Reinforcement Learning Environments," 2024.

\section{Research Plan}
Our plan to tackle this problem has four steps which will be described in the following:
\begin{enumerate}
  \item Setup of the simulation environment and integration with the machine learning framework.
  \item Defining the geometry of the robot and training a proof of concept controller for a simple
        and static robot state
  \item Training a controller to keep the robot in its upright position while allowing for changing heights
  \item Improving the controller further by introducing perturbations such as shifting the center of
        gravity.
\end{enumerate}

The setup of the simulation environment will probably utilize pybullet or gazebo as a simulation
environment. Here an interface for controlling the bot and interacting with the simulation environment
will be needed. On top of that an integration with Gymnasium, a popular reinforcement learning
framework will be needed. This involves setting up a gym environment and implementing the necessary
feedback mechanisms for calculating the reward function.

With the learning environment set up the next step is to define the geometry and capabilities of the
bot, which can be found in the paper referenced above. With a basic reward function we can then try
to train a policy which is able to keep the robot upright, just like an inverted pendulum.

After this baseline is established we will move forward to training a more general policy aiming for
stability in different height configurations of the robot.

In the end we might be able to extend our goals and try to make the controller robust by training
the policy on a mildly randomized center of gravity and actively changing robot heights and possibly
other features.
\end{document}